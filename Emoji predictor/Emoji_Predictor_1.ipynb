{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4005c30e-a9bc-4bc7-8142-92b55c8d590f",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803ce10b-72a0-4f42-9eb9-3b9817a87e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b504e-2723-4188-a6a6-14b5edc4bb20",
   "metadata": {},
   "source": [
    "#### Step 1: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d84734-ec1c-4239-ba9d-f4ef683aa912",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"G:\\\\CAP_Guvi\\\\train_emotions.csv\")\n",
    "val_df = pd.read_csv(\"G:\\\\CAP_Guvi\\\\val_emotions.csv\")\n",
    "test_df = pd.read_csv(\"G:\\\\CAP_Guvi\\\\test_emotions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6317a5-4d3f-4cf6-b1e4-11b2db791395",
   "metadata": {},
   "source": [
    "#### Step 2: Text Cleaning Function and Apply Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3907e40e-8f50-44e0-9393-815405fb32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)     # Remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)     # Remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuations\n",
    "    text = re.sub(r\"\\d+\", \"\", text)      # Remove digits\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df['sentence'] = df['sentence'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ba062-c3a6-48a8-accb-e5029a4d2fc0",
   "metadata": {},
   "source": [
    "#### Step 3: Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66de3fa8-cd8c-47d6-ae78-69d8962bd374",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_df['label'] = label_encoder.fit_transform(train_df['emotion'])\n",
    "val_df['label'] = label_encoder.transform(val_df['emotion'])\n",
    "test_df['label'] = label_encoder.transform(test_df['emotion'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31007145-854d-4340-a1e7-efba4fa215f7",
   "metadata": {},
   "source": [
    "#### Step 5: Tokenization and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32eaee3a-ba54-44ce-80da-945bcbebc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit tokenizer only on training data\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df['sentence'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['sentence'])\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_df['sentence'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['sentence'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(len(seq) for seq in X_train_seq)  # You can also use a fixed number like 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "y_test = test_df['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff698877-621a-4ec7-9093-25a885478ab5",
   "metadata": {},
   "source": [
    "#### Save Tokenizer and LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be456135-fea8-418a-ad44-e05dd9e5b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fdc026-eb94-4063-8364-e73c78c89786",
   "metadata": {},
   "source": [
    "#### Model Architecture:1D CNN + Global Max Pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83819c3b-2935-4b64-8a92-b1124f01eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 101, 128)          6230144   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 97, 128)           82048     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6320773 (24.11 MB)\n",
      "Trainable params: 6320773 (24.11 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 128\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X_train_pad.shape[1]))\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c1a16-b245-493e-856f-0023d8cb9de4",
   "metadata": {},
   "source": [
    "####  Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a532a-f0b8-43f1-82e6-2dd10c24ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,                  \n",
    "    validation_data=(X_val_pad, y_val),   \n",
    "    epochs=10,                            \n",
    "    batch_size=334,                       \n",
    "    callbacks=[early_stop],               \n",
    "    verbose=1                             \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675d222-c439-4b48-9c4a-c34f091a12b3",
   "metadata": {},
   "source": [
    "#### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff6b93a-e2a4-4b27-8599-760423143ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'emotion_cnn_model.h5'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"emotion_cnn_model.h5\")\n",
    "print(\"âœ… Model saved as 'emotion_cnn_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506d0bc-30e2-46cc-8d90-f9144808e10d",
   "metadata": {},
   "source": [
    "#### Classification Report of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "116de08b-0966-479e-874f-bdc26856d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361/2361 [==============================] - 12s 5ms/step\n",
      "ðŸ“Š Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.93      0.95      0.94     14830\n",
      "        fear       0.93      0.95      0.94     12413\n",
      "         joy       0.97      0.92      0.94     21460\n",
      "        love       0.84      0.94      0.89      8639\n",
      "         sad       0.97      0.95      0.96     18179\n",
      "\n",
      "    accuracy                           0.94     75521\n",
      "   macro avg       0.93      0.94      0.93     75521\n",
      "weighted avg       0.94      0.94      0.94     75521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Predict class probabilities\n",
    "y_pred_probs = model.predict(X_test_pad)\n",
    "\n",
    "# Get predicted class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "target_names = label_encoder.classes_\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "print(\"ðŸ“Š Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Optional: Save report to a text file\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea55676-538e-4516-a20e-2eadb146a470",
   "metadata": {},
   "source": [
    "#### To avoid loading it with pickle and instead use a JSON format, tokenizer.pkl is saved as json file, which is more stable and version-independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5ac78a-8fc7-4f76-b17e-020683f79cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizer converted and saved as JSON.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "# Step 1: Load the tokenizer from the .pkl file\n",
    "with open(\"C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Step 2: Convert tokenizer to JSON\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "# Step 3: Save JSON to a file\n",
    "with open(\"C:\\\\Users\\\\HP\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\tokenizer.json\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(tokenizer_json)\n",
    "\n",
    "print(\"âœ… Tokenizer converted and saved as JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
